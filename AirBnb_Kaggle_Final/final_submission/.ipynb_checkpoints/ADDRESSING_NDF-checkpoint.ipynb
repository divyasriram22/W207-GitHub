{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDRESSING THE NDFs\n",
    "This notebook will attempt to hardcode rules that address to if-and-only-if relationship between Date of First Booking and NDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import Statements\n",
    "\n",
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Numpy libraries\n",
    "from numpy import nan\n",
    "\n",
    "# SK-learn library for preprocessing\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read in csv and create arrays\n",
    "users_train_raw = pd.read_csv('../train_dev_data/train_w_sessions.csv', index_col=0)   # Note: this is the user data with new paramters from sessions\n",
    "test = pd.read_csv('../train_dev_data/test_w_sessions.csv', index_col=0)\n",
    "demographics = pd.read_csv('../zip_files/age_gender_bkts.csv.zip')\n",
    "countries = pd.read_csv('../zip_files/countries.csv.zip') \n",
    "\n",
    "users_train_raw = users_train_raw.drop('country_destination', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88908"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing all NDF's from training data\n",
    "train_no_ndf = users_train_raw[pd.isnull(users_train_raw.date_first_booking) !=  True]\n",
    "train_no_ndf\n",
    "train_no_ndf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "len(train_no_ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73815"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a new version of \n",
    "train_no_ndf_2 = train_no_ndf.copy()\n",
    "train_no_ndf_2 = train_no_ndf_2.drop('date_first_booking', 1)  #Removing date of first booking\n",
    "train_no_ndf_2 = train_no_ndf_2.drop('signup_delta', 1)   # Removing Delta between signup and booking\n",
    "train_no_ndf_2 = users_train_raw[pd.isnull(users_train_raw.number_visits) !=  True] # REmoving all rows that predated sessions information\n",
    "train_no_ndf_2.reset_index(drop=True, inplace=True) \n",
    "\n",
    "len(train_no_ndf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73815"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_old = users_train_raw[pd.isnull(users_train_raw.number_visits) !=  True]\n",
    "train_no_old = train_no_old.drop('date_first_booking', 1)  #Removing date of first booking\n",
    "train_no_ndf_2 = train_no_old.drop('signup_delta', 1)   # Removing Delta between signup and booking\n",
    "\n",
    "len(train_no_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(train_no_ndf.shape[0]))\n",
    "len(shuffle)\n",
    "x = train_no_ndf.reindex(shuffle)\n",
    "\n",
    "# encode all values in numbers \n",
    "y = pd.DataFrame()\n",
    "for column in list(x):\n",
    "    y[column] = pd.factorize(x[column], sort=True)[0]\n",
    "\n",
    "# split out labels from data frame\n",
    "data, labels = preprocessing.normalize(np.asarray(y)[:,:-1]), np.asarray(y)[:,-1]\n",
    "\n",
    "# Split into train and dev.\n",
    "dev_data, dev_labels = data[:8000], labels[:8000]\n",
    "train_data, train_labels = data[8000:], labels[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 21)) while a minimum of 1 is required by the normalize function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0f8f73f8e612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# split out labels from data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdata_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Split into train and dev.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     X = check_array(X, sparse_format, copy=copy, warn_on_dtype=True,\n\u001b[0;32m-> 1344\u001b[0;31m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    414\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 416\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 21)) while a minimum of 1 is required by the normalize function."
     ]
    }
   ],
   "source": [
    "# Shuffle data - Attempt 2 \n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(train_no_ndf_2.shape[0]))\n",
    "len(shuffle)\n",
    "x = train_no_ndf.reindex(shuffle)\n",
    "\n",
    "# encode all values in numbers \n",
    "y = pd.DataFrame()\n",
    "for column in list(x):\n",
    "    y[column] = pd.factorize(x[column], sort=True)[0]\n",
    "\n",
    "# split out labels from data frame\n",
    "data_2, labels_2 = preprocessing.normalize(np.asarray(y)[:,:-1]), np.asarray(y)[:,-1]\n",
    "\n",
    "# Split into train and dev.\n",
    "dev_data_2, dev_labels_2 = data_2[:8000], labels_2[:8000]\n",
    "train_data_2, train_labels_2 = data_2[8000:], labels_2[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle data - Attempt 3\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(np.arange(train_no_old.shape[0]))\n",
    "len(shuffle)\n",
    "x = train_no_ndf.reindex(shuffle)\n",
    "\n",
    "# encode all values in numbers \n",
    "y = pd.DataFrame()\n",
    "for column in list(x):\n",
    "    y[column] = pd.factorize(x[column], sort=True)[0]\n",
    "\n",
    "# split out labels from data frame\n",
    "data_3, labels_3 = preprocessing.normalize(np.asarray(y)[:,:-1]), np.asarray(y)[:,-1]\n",
    "\n",
    "# Split into train and dev.\n",
    "dev_data_3, dev_labels_3 = data_3[:8000], labels_3[:8000]\n",
    "train_data_3, train_labels_3 = data_3[8000:], labels_3[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.676 (+/-0.001) for {'alpha': 0.0}\n",
      "0.720 (+/-0.001) for {'alpha': 0.0001}\n",
      "0.720 (+/-0.001) for {'alpha': 0.001}\n",
      "0.720 (+/-0.001) for {'alpha': 0.01}\n",
      "0.720 (+/-0.001) for {'alpha': 0.1}\n",
      "0.720 (+/-0.000) for {'alpha': 0.5}\n",
      "0.720 (+/-0.000) for {'alpha': 1.0}\n",
      "0.720 (+/-0.000) for {'alpha': 2.0}\n",
      "0.721 (+/-0.000) for {'alpha': 10.0}\n",
      "('\\nOptimized Parameters: ', BernoulliNB(alpha=10.0, binarize=0.0, class_prior=None, fit_prior=True))\n",
      "optimized accuracy: 0.7220\n",
      "('Best alpha:', {'alpha': 10.0})\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes - For Training/Dev that have all NDF removed but does still have date_first_booking\n",
    "# finding the best alpha\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "\n",
    "bnb_clf = BernoulliNB()\n",
    "bnb = GridSearchCV(estimator=bnb_clf, param_grid=[alphas], cv=5, scoring=\"accuracy\", refit=True)\n",
    "bnb.fit(train_data, train_labels)\n",
    "for params, mean_score, scores in bnb.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" %(mean_score, scores.std()/2, params))\n",
    "\n",
    "print(\"\\nOptimized Parameters: \", bnb.best_estimator_)\n",
    "print(\"optimized accuracy: %.4f\" %bnb.score(dev_data, dev_labels))\n",
    "print(\"Best alpha:\", bnb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Bernoulli Naive Bayes - For Training/Dev that have all NDF & pre-session removed, as well as booking date and booking delta columns\n",
    "# finding the best alpha\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "\n",
    "bnb_clf_2 = BernoulliNB()\n",
    "bnb_2 = GridSearchCV(estimator=bnb_clf_2, param_grid=[alphas], cv=5, scoring=\"accuracy\", refit=True)\n",
    "bnb_2.fit(train_data_2, train_labels_2)\n",
    "for params, mean_score, scores in bnb_2.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" %(mean_score, scores.std()/2, params))\n",
    "\n",
    "print(\"\\nOptimized Parameters: \", bnb_2.best_estimator_)\n",
    "print(\"optimized accuracy: %.4f\" %bnb_2.score(dev_data_2, dev_labels_2))\n",
    "print(\"Best alpha:\", bnb_2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/cross_validation.py:552: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=5.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/naive_bayes.py:801: RuntimeWarning: divide by zero encountered in log\n",
      "  self.feature_log_prob_ = (np.log(smoothed_fc) -\n",
      "/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/naive_bayes.py:820: RuntimeWarning: divide by zero encountered in log\n",
      "  neg_prob = np.log(1 - np.exp(self.feature_log_prob_))\n",
      "/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/naive_bayes.py:823: RuntimeWarning: invalid value encountered in add\n",
      "  jll += self.class_log_prior_ + neg_prob.sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815 (+/-0.001) for {'alpha': 0.0}\n",
      "0.843 (+/-0.001) for {'alpha': 0.0001}\n",
      "0.843 (+/-0.001) for {'alpha': 0.001}\n",
      "0.843 (+/-0.001) for {'alpha': 0.01}\n",
      "0.843 (+/-0.001) for {'alpha': 0.1}\n",
      "0.843 (+/-0.001) for {'alpha': 0.5}\n",
      "0.843 (+/-0.001) for {'alpha': 1.0}\n",
      "0.843 (+/-0.001) for {'alpha': 2.0}\n",
      "0.844 (+/-0.001) for {'alpha': 10.0}\n",
      "('\\nOptimized Parameters: ', BernoulliNB(alpha=10.0, binarize=0.0, class_prior=None, fit_prior=True))\n",
      "optimized accuracy: 0.8384\n",
      "('Best alpha:', {'alpha': 10.0})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gochs/anaconda/envs/py27/lib/python2.7/site-packages/sklearn/grid_search.py:438: ChangedBehaviorWarning: The long-standing behavior to use the estimator's score function in GridSearchCV.score has changed. The scoring parameter is now used.\n",
      "  ChangedBehaviorWarning)\n"
     ]
    }
   ],
   "source": [
    "# Bernoulli Naive Bayes - For Training/Dev that have all NDF & pre-session removed, as well as booking date and booking delta columns\n",
    "# finding the best alpha\n",
    "alphas = {'alpha': [0.0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 10.0]}\n",
    "\n",
    "bnb_clf_3 = BernoulliNB()\n",
    "bnb_3 = GridSearchCV(estimator=bnb_clf_3, param_grid=[alphas], cv=5, scoring=\"accuracy\", refit=True)\n",
    "bnb_3.fit(train_data_3, train_labels_3)\n",
    "for params, mean_score, scores in bnb_3.grid_scores_:\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" %(mean_score, scores.std()/2, params))\n",
    "\n",
    "print(\"\\nOptimized Parameters: \", bnb_3.best_estimator_)\n",
    "print(\"optimized accuracy: %.4f\" %bnb_3.score(dev_data_3, dev_labels_3))\n",
    "print(\"Best alpha:\", bnb_3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Processing Test data like Train & Dev\n",
    "\n",
    "# test_data = pd.DataFrame()\n",
    "# for column in list(test):\n",
    "#     test_data[column] = pd.factorize(test[column], sort=True)[0]\n",
    "    \n",
    "# test_labels = []\n",
    "# for label in test.user_id:\n",
    "#     test_labels.append(label)\n",
    "    \n",
    "# test_data = preprocessing.normalize(np.asarray(test_data)[:,:])\n",
    "\n",
    "# # NB Model on Test - Just for Funsies\n",
    "# test_prediction = bnb.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing all NDF's from training data\n",
    "test_updated = test.drop('date_first_booking', 1)  #Removing date of first booking\n",
    "test_updated = test.drop('signup_delta', 1)   # Removing Delta between signup and booking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Processing Test data like Train & Dev\n",
    "\n",
    "test_data_2 = pd.DataFrame()\n",
    "for column in list(test_updated):\n",
    "    test_data_2[column] = pd.factorize(test_updated[column], sort=True)[0]\n",
    "    \n",
    "test_labels_2 = []\n",
    "for label in test_updated.user_id:\n",
    "    test_labels_2.append(label)\n",
    "    \n",
    "test_data_2 = preprocessing.normalize(np.asarray(test_data_2)[:,:])\n",
    "\n",
    "# NB Model on Test - Just for Funsies\n",
    "# test_prediction_2 = bnb_2.predict(test_data_2)\n",
    "test_prediction_3 = bnb_3.predict(test_data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# submission = pd.DataFrame(data=test_prediction,index=test_labels)\n",
    "# submission.columns.name = 'id'\n",
    "# submission.rename(columns={0:'country'}, inplace=True)\n",
    "# vals_to_replace = { 0 : 'AU', 1 : 'CA', 2 : 'DE', 3 : 'ES', 4 : 'FR', 5 : 'GB', 6 : 'IT', 7 : 'NDF', 8 : 'NL', 9 : 'PT', 10 : 'US', 11 : 'other' }\n",
    "# submission['country'] = submission['country'].map(vals_to_replace)\n",
    "# submission.to_csv('airbnb_submission_1.csv',sep=',',index_label='id')\n",
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Submission for data with no NDF in training data, no data_first_reservation, no date_Delta, as well as all pre-sessions data removed\n",
    "submission = pd.DataFrame(data=test_prediction_2,index=test_labels_2)\n",
    "submission.columns.name = 'id'\n",
    "submission.rename(columns={0:'country'}, inplace=True)\n",
    "vals_to_replace = { 0 : 'AU', 1 : 'CA', 2 : 'DE', 3 : 'ES', 4 : 'FR', 5 : 'GB', 6 : 'IT', 7 : 'NDF', 8 : 'NL', 9 : 'PT', 10 : 'US', 11 : 'other'}\n",
    "submission['country'] = submission['country'].map(vals_to_replace)\n",
    "submission = submission.fillna(value='NDF')\n",
    "submission.to_csv('airbnb_submission_1.csv',sep=',',index_label='id')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5uwns89zht</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jtl0dijy2j</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xx0ulgorjt</th>\n",
       "      <td>NL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6c6puo6ix0</th>\n",
       "      <td>NL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>czqhjk3yfe</th>\n",
       "      <td>NL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>szx28ujmhf</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guenkfjcbq</th>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tkpq0mlugk</th>\n",
       "      <td>NL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3xtgd5p9dn</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>md9aj22l5a</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gg3eswjxdf</th>\n",
       "      <td>NL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fyomoivygn</th>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iq4kkd5oan</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6k1xls6x5j</th>\n",
       "      <td>NL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jodmb2ok1f</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eq6fy0m4vc</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yq4i7nfh6l</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q5pibqdous</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i0sc6d3j8s</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>br5mcrsqzn</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rddbczuxx1</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glck7hlmzz</th>\n",
       "      <td>NL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sxpkaxep8n</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sr4ntmalz2</th>\n",
       "      <td>NL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f6wueq1ccn</th>\n",
       "      <td>NL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ovc6nwn6mj</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n10skstp90</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5jrbdigmv4</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d45nngmojp</th>\n",
       "      <td>PT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y0frb6t1kq</th>\n",
       "      <td>NL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bsv2ev628t</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06echc56pl</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>niqgaye2ov</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61iwzuhw6e</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gks02el96u</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v4r1161l0r</th>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p1clbqd0o6</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ozb2z0km6l</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w3e3sp6i70</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q5bxbq0asg</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1xa5t3t0la</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuvz7gfpjz</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpusl6ppgf</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpijioh4eh</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3ptlvdxss9</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f9a1ncjnrg</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kofaz2kh70</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6xrmom7hjo</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cg9wqgnad2</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jg618z94wo</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u7lv3glv6y</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o6ofmozucx</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wcw7xggeqp</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m22pw2pkxr</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8yvhec201j</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cv0na2lf5a</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zp8xfonng8</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa6260ziny</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87k0fy4ugm</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9uqfg8txu3</th>\n",
       "      <td>NDF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62096 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id         country\n",
       "5uwns89zht     NDF\n",
       "jtl0dijy2j     NDF\n",
       "xx0ulgorjt      NL\n",
       "6c6puo6ix0      NL\n",
       "czqhjk3yfe      NL\n",
       "szx28ujmhf     NDF\n",
       "guenkfjcbq      US\n",
       "tkpq0mlugk      NL\n",
       "3xtgd5p9dn     NDF\n",
       "md9aj22l5a     NDF\n",
       "gg3eswjxdf      NL\n",
       "fyomoivygn      US\n",
       "iq4kkd5oan     NDF\n",
       "6k1xls6x5j      NL\n",
       "jodmb2ok1f     NDF\n",
       "eq6fy0m4vc     NDF\n",
       "yq4i7nfh6l     NDF\n",
       "q5pibqdous     NDF\n",
       "i0sc6d3j8s     NDF\n",
       "br5mcrsqzn     NDF\n",
       "rddbczuxx1     NDF\n",
       "glck7hlmzz      NL\n",
       "sxpkaxep8n     NDF\n",
       "sr4ntmalz2      NL\n",
       "f6wueq1ccn      NL\n",
       "ovc6nwn6mj     NDF\n",
       "n10skstp90     NDF\n",
       "5jrbdigmv4     NDF\n",
       "d45nngmojp      PT\n",
       "y0frb6t1kq      NL\n",
       "...            ...\n",
       "bsv2ev628t     NDF\n",
       "06echc56pl     NDF\n",
       "niqgaye2ov     NDF\n",
       "61iwzuhw6e     NDF\n",
       "gks02el96u     NDF\n",
       "v4r1161l0r   other\n",
       "p1clbqd0o6     NDF\n",
       "ozb2z0km6l     NDF\n",
       "w3e3sp6i70     NDF\n",
       "q5bxbq0asg     NDF\n",
       "1xa5t3t0la     NDF\n",
       "zuvz7gfpjz     NDF\n",
       "gpusl6ppgf     NDF\n",
       "gpijioh4eh     NDF\n",
       "3ptlvdxss9     NDF\n",
       "f9a1ncjnrg     NDF\n",
       "kofaz2kh70     NDF\n",
       "6xrmom7hjo     NDF\n",
       "cg9wqgnad2     NDF\n",
       "jg618z94wo     NDF\n",
       "u7lv3glv6y     NDF\n",
       "o6ofmozucx     NDF\n",
       "wcw7xggeqp     NDF\n",
       "m22pw2pkxr     NDF\n",
       "8yvhec201j     NDF\n",
       "cv0na2lf5a     NDF\n",
       "zp8xfonng8     NDF\n",
       "fa6260ziny     NDF\n",
       "87k0fy4ugm     NDF\n",
       "9uqfg8txu3     NDF\n",
       "\n",
       "[62096 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submission#3 : for data with no NDF in training data, no data_first_reservation, no date_Delta, as well as all pre-sessions data removed\n",
    "submission = pd.DataFrame(data=test_prediction_3,index=test_labels_2)\n",
    "submission.columns.name = 'id'\n",
    "submission.rename(columns={0:'country'}, inplace=True)\n",
    "vals_to_replace = { 0 : 'AU', 1 : 'CA', 2 : 'DE', 3 : 'ES', 4 : 'FR', 5 : 'GB', 6 : 'IT', 7 : 'NDF', 8 : 'NL', 9 : 'PT', 10 : 'US', 11 : 'other'}\n",
    "submission['country'] = submission['country'].map(vals_to_replace)\n",
    "submission = submission.fillna(value='NDF')\n",
    "submission.to_csv('airbnb_submission_1.csv',sep=',',index_label='id')\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
